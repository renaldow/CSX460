---
title: "Sensitivity and Specificity"
author: "Renaldo Williams"
date: "October 5, 2015"
output: html_document
---


## Readings

***APM***

- ***Chapter 5 Measuring Performance in Regression Models*** (esp. ***5.2 The Variance Bias Trade-Off***)  (5 pages)
- ***Chapter 11 Measuring Performance in Classification Models*** (~20 pages)
- ***Chapter 7.4 K-Nearest Neighbors (regression)*** (2 pages)
- ***Chapter 13.5 K-Nearest Neighbors (classification)*** (3 pages)


```{r, echo=FALSE, results='hide', warning=FALSE }

packs <-  c('ggplot2', 'magrittr', 'dplyr', 'caret', 'AppliedPredictiveModeling')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

.. = NULL  # For Aesthetics

```


## EXERCISE 1: Resampling

`x` is a random variable. We want to not only know what the `mean(x)` is but want to calculate the uncertainty of `mean(x)`.  Measuring the uncertainty requires repeated measurements of `mean(x)`.

1. Calculate the mean of `x`.
2. Calculte the `sd( mean(x) )` using the **using 10-fold cross-validation**.  Create your own folds, show your work. (An example is for the Bootstrap is given as a hint. )


```{r}
#set.seed(1) 
#x <- runif(20,1,20) #take random number from uniform distribution take 20 numbers from 1 - 20
#x_mean = mean(x)

#k=10

# CROSS-VALIDATION
# Take 
x <- runif(20,1,20) #take random number from uniform distribution take 20 numbers from 1 - 20
x_mean <- numeric()
for(k in 1:10) {
  x_mean <- append(x_mean, sample(x, replace=TRUE) %>% mean)
}
sd_cv <- x_mean %>% sd # This is the standard deviation of the mean of k folds

# BOOTSTRAP (EXAMPLE)
# shorthand function
# (RW) This takes 20 samples from vector x and finds the mean. It does it 10 times since k = 10. It then takes all 10 means and finds the standard deviation.
sd_boot <- sapply(1:k, function(i) sample(x,replace=TRUE) %>% mean ) %>% sd #take my vector, x, and take samples equal to size of vector

# sort(sample(x,replace=TRUE))
```


- sd_cv   is: `r sd_cv`
- sd_boot is: `r sd_boot`



# Exercise 2: Binomial Metrics

Here's a really simple Model of Versicolor iris based on the **iris** data :

```{r}
set.seed(1)

data(iris)

qplot( data=iris, x=Petal.Length, y=Sepal.Length, color=Species )

# Create Dependent Variable
iris$Versicolor <- 
  ifelse( iris$Species == 'versicolor', "versicolor", "other" ) %>% as.factor

iris$Species = NULL 
nrow(iris)

#RW take sample 75 samples from 1 - 150, save row id's
wh <- sample.int( nrow(iris), size=nrow(iris)/2 ) # this samples rows from 1 to n=150. Only store half in wh

train <- iris[ wh,] # 75 items for training
test <- iris[ -wh, ] # 75 items for testing


fit.glm <- glm( Versicolor ~ . - Sepal.Length, data=train, family=binomial )
fit.pred <- predict(fit.glm, test)

summary(fit.glm)
summary(fit.pred)
summary()

table(test$Versicolor)
table(fit.pred$Versicolor)

cor(iris[c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")])

install.packages("psych")
library("psych")

pairs.panels(iris[c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")])

str(iris)
```


Use the models to and write functions to calculate:

* Prevalence 
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

The functions should take two logical vectors of the same length, `y` and `yhat`

```{r}

prevalence = function(y,yhat) {}
accuracy   =  .. 
error_rate = ..
tpr = ..
fpr = ..      # See Example
tnr = ..
sensitivity = ..
specificity = ..
recall = .. 
precision = ..

# EXAMPLE: fpr
# The FPR is THE NUMBER OF FALSE POSITIVES / NEGATIVES (TN+FP)

threshold = 0.5 
y = test$Versicolor == 'versicolor' # y is the positive sample in this case

# (RW) if probability is greater than .5 then TRUE meaning is versicolor
yhat = predict(fit.glm, test, type="response") > threshold
#yhat2 = predict(fit.glm, test, type="response")

#table(yhat2)

a <- data.frame(y)
#a$yhat <- data.frame(yhat)

a$y <- y
a$yhat <- yhat
a$test_non_equality <- (y != yhat)
a$y_equal_yhat <- (y == yhat)



#table(y, yhat)



install.packages("gmodels")
library("gmodels")

# important to note: CrossTable uses y, yhat (actual, predicted) as order of arguments BUT confusionMatrix uses yhat, y (predicted, actual) as order of operations
CrossTable(y, yhat, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual versicolor','predicted versicolor'))

#*Remember confusion matrix uses (predicted, actual) as order of operations
confusionMatrix(yhat, y, positive = "TRUE")

table(y)
table(a$test_non_equality)

#(y != yhat) will create a vector where items that are true are where actual and predicted arent true, so this will count FP and FN.
# y & (...) will count only and the true items (which are FP and FN) and only equate to true for items that are actual TRUE, hence counting FP / Total Negatives
#sum(y & (y != yhat))

#PREVALENCE
prevalence <- function(y,yhat) {
  actual_p <- sum(y)
  total <- length(y)
  result <- actual_p / total # Total Actual P / Total
  result
}

prevalence(y,yhat)

#FPR
fpr <- function(y,yhat)
  sum(y & (y != yhat) ) / # FP
  sum(! y)                # Total Actual N

fpr(y,yhat)

#TPR
tpr <- function(y,yhat)
  sum(y & (y == yhat) ) / # TP
  sum(y)                # Total Actual P

tpr(y,yhat)

#TNR
tnr <- function(y,yhat)
  sum(!y & (y == yhat) ) / # TN
  sum(!y)                # Total Actual N

tnr(y,yhat)

#Sensitivity
sensitivity <- function(y,yhat) {
  tp <- sum(y & (y == yhat))
  fn <- sum(!y & (y != yhat))
  result <- tp / (tp + fn) # TP / TP + FN
  result
}
                
sensitivity(y,yhat)

#Specificity
specificity <- function(y,yhat) {
  tn <- sum(!y & (y == yhat))
  actual_f <- sum(!y)
  result <- tn / actual_f # TN / TN + FP
  result
}

specificity(y,yhat)

#Precision
precision <- function(y,yhat) {
  tp <- sum(y & (y == yhat))
  fp <- sum(y & (y != yhat))
  result <- tp / (tp + fp) # TP / TP + FP
  result
}

precision(y,yhat)

#Recall
recall <- function(y,yhat) {
  tp <- sum(y & (y == yhat))
  fn <- sum(!y & (y != yhat))
  result <- tp / (tp + fn) # TP / TP + FN
  result
}

recall(y,yhat)

#posPredValue(yhat, y, positive = "TRUE")

#ACCURACY
accuracy = function(y,yhat)
  sum(y == yhat) / # TP + TN
  length(y)                # Total N

accuracy(y,yhat)

#ERROR RATE
error_rate = 1 - accuracy(y,yhat)
error_rate


```

- What is wrong with the modeling approach used?

We are using a linear model to predict a class of versicolor instead of using linear model to predit a number. Linear models are used to predict numbers. The code above gets around this by using probabilities >.5 to mean versicolor and <.5 to mean not versicolor, but a classification model should be used instead of linear model.



